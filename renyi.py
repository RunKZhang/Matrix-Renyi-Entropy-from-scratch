import numpy as np
from scipy.spatial.distance import pdist, squareform

def RBF(X, sigma):
    """
    Using Radial Basis Function to calculate K(x_i, x_j)
    """
    # GPT version
    # np.exp(-np.linalg.norm(x-y)**2 / (2 * sigma**2))

    K = squareform(pdist(X, metric='euclidean'))
    K = K ** 2
    K = -1/(2 * sigma**2)*K
    K = np.exp(K)
    return K

def Gram(K):
    """
    Genarate the Gram matrix A in the paper.
    K is the Gram matrix generated by kernel methods, and normalization is in this.
    """
    A = K / (np.sqrt(np.outer(np.diag(K), np.diag(K)))* K.shape[0])
    return A
    
class Renyi_Base():
    def __init__(self, A):
        self.A = A
        # self.samples = samples
        # self.sigma = sigma

        # calculate A,B,...... as in papers.
        # self.K = RBF(samples, sigma)
        # self.A = Gram(self.K)
    


class Renyi_Calculator():
    def __init__(self, key, alpha, A, B=None) -> None:
        self.A = A
        self.B = B
        self.key = key
        self.alpha = alpha
    
    def entropy(self, mat_A):
        """
        base class for S_alpha_A
        return S_alpha_A 
        Compute Renyi's Entropy, mat_A shape: [n*n]
        """
        eigenvalues  = np.linalg.eigvalsh(mat_A)

        # print(eigenvalues)
        # A_alpha = np.linalg.matrix_power(mat_A, alpha)

        # trace_A_alpha = np.trace(A_alpha)
        # min_eigenvalue = np.min(eigenvalues)
        # if min_eigenvalue < 0:
        #     adjustment = abs(min_eigenvalue) + 1e-10
        #     A_adjusted = mat_A + adjustment * np.eye(mat_A.shape[0])
        # else:
        #     A_adjusted = mat_A

        # eigenvalues = np.linalg.eigvalsh(A_adjusted)
        
        eigenvalues[eigenvalues < 0] = 0
        # if not np.all(eigenvalues >= 0):
        #     print('Not all eigenvalues more than 0')
        # summation = np.sum(eigenvalues ** alpha)
        # S_alpha_A = (1 / (1 - alpha)) * np.log2(trace_A_alpha)
        # print(f'eigenvalues:{eigenvalues.shape}')
        S_alpha_A = 1/ (1 - self.alpha) * np.log2(np.sum(eigenvalues**self.alpha))
        # print('I am here')
        # use numpy now, maybe torch is more fast since cuda.
        # eigenvalues = np.linalg.eigvalsh(mat_A)
        # print(f'eigenvalues:{eigenvalues}')
        # S_alpha_A = 1/ (1 - self.alpha) * np.log2(np.sum(eigenvalues**self.alpha))
        return S_alpha_A
    
    def fit(self):
        """
        switch():
        """
        # print(f'key:{self.key}')
        # switch={
        #     'entropy': self.entropy(self.A, self.alpha),
        #     'joint_entropy': self.joint_entropy(self.A, self.alpha),
        #     'mutual_information': self.mutual_information(self.A, self.B, self.alpha),
        #     'total_correlation': self.total_correlation(self.A, self.alpha)
        # }
        if self.key == 'entropy':
            return self.entropy(self.A)
        elif self.key == 'joint_entropy': 
            return self.joint_entropy(self.A)
        elif self.key == 'mutual_information':
            return self.mutual_information(self.A, self.B)
        elif self.key == 'total_correlation': 
            return self.total_correlation(self.A)

        # return switch.get(self.key)
        # if key == 'joint_entropy':
            # suppose samples in format num_variables * sample_length
            # A_list = []
            # for sample in self.samples[0:]:
            #     print(sample.shape)
                # A_list.append(Gram(RBF(sample)))
                
            # kernel_matrics = np.concatenate(A_list, axis=0)
            # A = self.joint_entropy()
    

    def joint_entropy(self, kernel_matrics):
        """
        Compute Renyi's Joint Entropy, the kernel_matrics are of (A_1, A_2, A_3, ..., A_k)
        kernel_matrics shape: [k, n, n].
        """
        # Compute the Hadamard product of kernel matrices
        A = kernel_matrics[0]
        for mat in kernel_matrics[1:]:
            A = np.multiply(A, mat) # Element-wise multiplication

        # print(f'A.shape:{A.shape}')
        # Normalize the combined matrix
        A = A / np.trace(A)

        S = self.entropy(A)

        return S
    
    def mutual_information(self, kernel_matrics_A, mat_B):
        """
        Compute Renyi's Mutual Information between the kernel_matrics are of (A_1, A_2, A_3, ..., A_k), and the B
        """
        A_total = kernel_matrics_A[0]
        for mat in kernel_matrics_A[1:]:
            A_total = np.multiply(A_total, mat)

        A_B = np.multiply(A_total, mat_B)
        A_total = A_total / np.trace(A_total)
        A_B = A_B / np.trace(A_B)

        I_alpha = self.entropy(mat_B) + self.entropy(A_total) - self.entropy(A_B)
        return I_alpha
    
    def total_correlation(self, kernel_matrics_A):
        """
        Compute Renyi's Total Correlation between (A_1, A_2, A_3, ..., A_k)
        """
        # Compute the Hadamard product of kernel matrices
        A = kernel_matrics_A[0]
        for mat in kernel_matrics_A[1:]:
            A = np.multiply(A, mat) # Element-wise multiplication        

        # Normalize the combined matrix
        A = A / np.trace(A)

        TC_alpha = -self.entropy(A)
        for mat in kernel_matrics_A[0:]:
            TC_alpha = TC_alpha + self.entropy(mat)
        
        return TC_alpha
        
